{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E73Pjkb2vtzw",
        "outputId": "122a98a4-9f6b-4e90-dd78-4b65d83d7175"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mdhm29D3vUii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bb4ce2-1187-4d2d-9a05-adf083a1f11e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, KFold,RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.ensemble import VotingRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor, BaggingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from math import pi\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Ignore all warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the raw data\n",
        "\n",
        "This step make sure that the train and test data is in the same page."
      ],
      "metadata": {
        "id": "x21vm-jjz4M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Base features\n",
        "base_features = test_df.drop(columns=['id']).columns\n",
        "test_id = test_df['id']\n",
        "\n",
        "# Concatenate train and test datasets\n",
        "train_df = pd.concat([train_df[base_features], train_df['orders']], axis=1)\n",
        "\n",
        "test_df=test_df[base_features]\n",
        "\n",
        "train_test_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n",
        "\n",
        "date_col = 'date'"
      ],
      "metadata": {
        "id": "L47DmAL-wM1I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### feature extraction and processing"
      ],
      "metadata": {
        "id": "yP_0u5RM0DDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def base_features_processing(df):\n",
        "\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "    df[\"year\"] = df[date_col].dt.year.fillna(-1)\n",
        "    df[\"month\"] = df[date_col].dt.month.fillna(-1)\n",
        "    df[\"day\"] = df[date_col].dt.day.fillna(-1)\n",
        "    df[\"day_of_week\"] = df[date_col].dt.dayofweek.fillna(-1)\n",
        "\n",
        "    df[\"week_of_year\"] = df[date_col].dt.isocalendar().week.fillna(-1)\n",
        "\n",
        "\n",
        "    df[\"quarter\"] = df[date_col].dt.quarter.fillna(-1)\n",
        "    df[\"is_month_start\"] = df[date_col].dt.is_month_start.astype(int).fillna(-1)\n",
        "    df[\"is_month_end\"] = df[date_col].dt.is_month_end.astype(int).fillna(-1)\n",
        "    df[\"is_quarter_start\"] = df[date_col].dt.is_quarter_start.astype(int).fillna(-1)\n",
        "    df[\"is_quarter_end\"] = df[date_col].dt.is_quarter_end.astype(int).fillna(-1)\n",
        "\n",
        "\n",
        "    # check if the holiday is close.\n",
        "    df['holiday_before'] = df['holiday'].shift(1).fillna(0).astype(int)\n",
        "    df['holiday_after'] = df['holiday'].shift(-1).fillna(0).astype(int)\n",
        "\n",
        "    # total number of holidays in the corresponding month of that row\n",
        "    df['total_holidays_month'] = df.groupby(['year', 'month'])['holiday'].transform('sum')\n",
        "    # the total number of days that shops were closed in the corresponding week of that row\n",
        "    df['total_shops_closed_week'] = df.groupby(['year', 'week_of_year'])['shops_closed'].transform('sum')\n",
        "\n",
        "    df.drop(date_col, axis=1, inplace=True)\n",
        "\n",
        "    # Replace null values in holiday_name with 'None'\n",
        "    df['holiday_name'].fillna('None', inplace=True)\n",
        "\n",
        "    # OneHotEncoding for holiday_name\n",
        "\n",
        "    enc = OneHotEncoder(sparse=False)\n",
        "    holiday_encoded = enc.fit_transform(df[['holiday_name']])\n",
        "\n",
        "    encoded_df = pd.DataFrame(holiday_encoded, columns=enc.get_feature_names_out(['holiday_name']))\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "    df.drop('holiday_name', axis=1, inplace=True)\n",
        "\n",
        "    # LabelEncoding for warehouse column;\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    df['warehouse'] = le.fit_transform(df['warehouse'])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Ynj84XgJm7uT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract some basic features\n",
        "train_test_df=base_features_processing(train_test_df)"
      ],
      "metadata": {
        "id": "xqTNL8cRnqpt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Work with seasonality"
      ],
      "metadata": {
        "id": "_XOYO9v0u24d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sine and cosine transformations\n",
        "# The reason we do this is that we want all cyclical patterns captured\n",
        "# capture seasonality\n",
        "def add_fourier_terms(df, year_k, week_k, day_k):\n",
        "    for k in range(1, year_k+1):\n",
        "        df['year_sin_'+str(k)] = df['year'] * np.sin(2 * pi * df['year'])\n",
        "        df['year_cos_'+str(k)] = df['year'] * np.cos(2 * pi * df['year'])\n",
        "    for k in range(1, week_k+1):\n",
        "        df['month_sin_'+str(k)] = df['month'] * np.sin(2 * pi * df['month'])\n",
        "        df['month_cos_'+str(k)] = df['month'] * np.cos(2 * pi * df['month'])\n",
        "    for k in range(1, day_k+1):\n",
        "        df['day_sin_'+str(k)] = df['day'] * np.sin(2 * pi * df['day'])\n",
        "        df['day_cos_'+str(k)] = df['day'] * np.cos(2 * pi * df['day'])\n",
        "    for k in range(1, day_k+1):\n",
        "        df['quarter'+str(k)] = df['quarter'] * np.sin(2 * pi * df['quarter'])\n",
        "        df['quarter'+str(k)] = df['quarter'] * np.cos(2 * pi * df['quarter'])\n",
        "\n",
        "add_fourier_terms(train_test_df, year_k= 5, week_k=5, day_k=5)"
      ],
      "metadata": {
        "id": "I1zCI6wmu2T5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Go back to where we start\n",
        "Convert the train_test_df to seperate dfs since we have done some basic feature transformation"
      ],
      "metadata": {
        "id": "w5QFiOF7wulA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groupby_columns=['warehouse', 'holiday', 'shops_closed']\n",
        "print('groupby_columns: ', groupby_columns)\n",
        "\n",
        "train_test_df_2=train_test_df.copy()\n",
        "\n",
        "# Convert the data back to train_df and test_df\n",
        "train_df_processed = train_test_df_2[~train_test_df_2['orders'].isnull()]\n",
        "\n",
        "#train_df_processed.dropna(inplace=True)\n",
        "\n",
        "test_df_processed = train_test_df_2[train_test_df_2['orders'].isnull()]\n",
        "\n",
        "\n",
        "test_df_processed = test_df_processed.drop(columns=['orders'])\n",
        "\n",
        "test_data_len=len(test_df_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iajGBNOTwC4x",
        "outputId": "ab40b8f2-123f-4bac-ea05-a4104b956687"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "groupby_columns:  ['warehouse', 'holiday', 'shops_closed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill Na to make sure\n",
        "train_df_processed=train_df_processed.fillna(train_df_processed.mean())\n",
        "test_df_processed=test_df_processed.fillna(test_df_processed.mean())"
      ],
      "metadata": {
        "id": "SRaSkFCewYm6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move target to the last column\n",
        "column_to_move = train_df_processed['orders']\n",
        "train_df_processed = train_df_processed.drop('orders', axis=1)\n",
        "train_df_processed = pd.concat([train_df_processed, column_to_move], axis=1)"
      ],
      "metadata": {
        "id": "lwdwmGnDwhm9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build up more features\n",
        "\t•    Interaction Features: Created to capture complex relationships between orders and other binary indicators (like holidays or shop closures).\n",
        "\t•\tCumulative Features: Added to understand the accumulation of orders over time within specific groups, which can help in capturing trends.\n",
        "\t•\tHandling Missing Data: Ensures that the dataset is clean and ready for modeling by filling or dropping missing values.\n",
        "\t•\tSorting: Organizes the data chronologically, which is especially important for time-sensitive models.\n",
        "\t•\tWarehouse Analysis: Counts the occurrences of each warehouse in the test set to potentially guide further processing or analysis."
      ],
      "metadata": {
        "id": "i1SrJEiazS6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_processed['orders_holiday'] = train_df_processed['orders'] * train_df_processed['holiday']\n",
        "train_df_processed['orders_wsh'] = train_df_processed['orders'] * train_df_processed['winter_school_holidays']\n",
        "\n",
        "train_df_processed['orders_sh'] = train_df_processed['orders'] * train_df_processed['school_holidays']\n",
        "\n",
        "train_df_processed['orders_shops_closed'] = train_df_processed['orders'] * train_df_processed['shops_closed']\n",
        "\n",
        "#train_df_processed['daily_avg']  = train_df_processed.groupby(['warehouse','day_of_week'])['orders'].transform('mean')\n",
        "#train_df_processed['monthly_avg'] = train_df_processed.groupby(['warehouse','month'])['orders'].transform('mean')\n",
        "\n",
        "train_df_processed['cumulative_orders'] = train_df_processed.groupby(groupby_columns)['orders'].cumsum()\n",
        "holiday_names=['holiday_name_1848 Revolution Memorial Day (Extra holiday)', 'holiday_name_2nd Christmas Day', \"holiday_name_All Saints' Day Holiday\", 'holiday_name_Christmas Eve', 'holiday_name_Cyrila a Metodej', 'holiday_name_Day of National Unity', 'holiday_name_Den boje za svobodu a demokracii', 'holiday_name_Den ceske statnosti', 'holiday_name_Den osvobozeni', 'holiday_name_Den vzniku samostatneho ceskoslovenskeho statu', 'holiday_name_Easter Monday', 'holiday_name_Good Friday', 'holiday_name_Independent Hungary Day', 'holiday_name_International womens day', 'holiday_name_Jan Hus', 'holiday_name_Labour Day', 'holiday_name_Memorial Day for the Martyrs of Arad', 'holiday_name_Memorial Day for the Victims of the Communist Dictatorships', 'holiday_name_Memorial Day for the Victims of the Holocaust', 'holiday_name_Memorial Day of the Republic', 'holiday_name_National Defense Day', 'holiday_name_New Years Day', 'holiday_name_None', 'holiday_name_Peace Festival in Augsburg', 'holiday_name_Reformation Day']\n",
        "train_df_processed=train_df_processed.fillna(train_df_processed.mean())\n",
        "train_df_processed.dropna(inplace=True)\n",
        "train_df_processed.sort_values(by=['year','month','day'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "xC1kNThGzhNg",
        "outputId": "46693d93-e5a1-4a42-b4ed-0ab04ec2592f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      warehouse  holiday  shops_closed  winter_school_holidays  \\\n",
              "0             4        0             0                       0   \n",
              "1193          0        0             0                       0   \n",
              "2386          5        0             0                       0   \n",
              "3579          6        0             0                       0   \n",
              "6186          1        0             0                       0   \n",
              "...         ...      ...           ...                     ...   \n",
              "2385          0        0             0                       0   \n",
              "3578          5        0             0                       0   \n",
              "4771          6        0             0                       0   \n",
              "5556          3        0             0                       0   \n",
              "6185          2        0             0                       0   \n",
              "\n",
              "      school_holidays  year  month  day  day_of_week  week_of_year  ...  \\\n",
              "0                   0  2020     12    5            5            49  ...   \n",
              "1193                0  2020     12    5            5            49  ...   \n",
              "2386                0  2020     12    5            5            49  ...   \n",
              "3579                0  2020     12    5            5            49  ...   \n",
              "6186                0  2020     12    5            5            49  ...   \n",
              "...               ...   ...    ...  ...          ...           ...  ...   \n",
              "2385                0  2024      3   15            4            11  ...   \n",
              "3578                0  2024      3   15            4            11  ...   \n",
              "4771                0  2024      3   15            4            11  ...   \n",
              "5556                0  2024      3   15            4            11  ...   \n",
              "6185                0  2024      3   15            4            11  ...   \n",
              "\n",
              "      quarter2  quarter3  quarter4  quarter5   orders  orders_holiday  \\\n",
              "0          4.0       4.0       4.0       4.0   6895.0             0.0   \n",
              "1193       4.0       4.0       4.0       4.0   6447.0             0.0   \n",
              "2386       4.0       4.0       4.0       4.0   4154.0             0.0   \n",
              "3579       4.0       4.0       4.0       4.0   4091.0             0.0   \n",
              "6186       4.0       4.0       4.0       4.0   4623.0             0.0   \n",
              "...        ...       ...       ...       ...      ...             ...   \n",
              "2385       1.0       1.0       1.0       1.0  10777.0             0.0   \n",
              "3578       1.0       1.0       1.0       1.0   7140.0             0.0   \n",
              "4771       1.0       1.0       1.0       1.0   6408.0             0.0   \n",
              "5556       1.0       1.0       1.0       1.0   6512.0             0.0   \n",
              "6185       1.0       1.0       1.0       1.0   1916.0             0.0   \n",
              "\n",
              "      orders_wsh  orders_sh  orders_shops_closed  cumulative_orders  \n",
              "0            0.0        0.0                  0.0             6895.0  \n",
              "1193         0.0        0.0                  0.0             6447.0  \n",
              "2386         0.0        0.0                  0.0             4154.0  \n",
              "3579         0.0        0.0                  0.0             4091.0  \n",
              "6186         0.0        0.0                  0.0             4623.0  \n",
              "...          ...        ...                  ...                ...  \n",
              "2385         0.0        0.0                  0.0          8364673.0  \n",
              "3578         0.0        0.0                  0.0          5916900.0  \n",
              "4771         0.0        0.0                  0.0          5416577.0  \n",
              "5556         0.0        0.0                  0.0          2646950.0  \n",
              "6185         0.0        0.0                  0.0           935292.0  \n",
              "\n",
              "[7340 rows x 85 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3b09eb4-9bcc-4471-a705-79ed09adf267\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>warehouse</th>\n",
              "      <th>holiday</th>\n",
              "      <th>shops_closed</th>\n",
              "      <th>winter_school_holidays</th>\n",
              "      <th>school_holidays</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>week_of_year</th>\n",
              "      <th>...</th>\n",
              "      <th>quarter2</th>\n",
              "      <th>quarter3</th>\n",
              "      <th>quarter4</th>\n",
              "      <th>quarter5</th>\n",
              "      <th>orders</th>\n",
              "      <th>orders_holiday</th>\n",
              "      <th>orders_wsh</th>\n",
              "      <th>orders_sh</th>\n",
              "      <th>orders_shops_closed</th>\n",
              "      <th>cumulative_orders</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>49</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6895.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6895.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>49</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6447.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6447.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2386</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>49</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4154.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4154.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3579</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>49</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4091.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4091.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6186</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>49</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4623.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4623.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2385</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2024</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10777.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8364673.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3578</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2024</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7140.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5916900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4771</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2024</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6408.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5416577.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5556</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2024</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6512.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2646950.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6185</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2024</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1916.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>935292.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7340 rows × 85 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3b09eb4-9bcc-4471-a705-79ed09adf267')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3b09eb4-9bcc-4471-a705-79ed09adf267 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3b09eb4-9bcc-4471-a705-79ed09adf267');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2394d8ae-8e55-4628-91aa-7f7bcde85c4f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2394d8ae-8e55-4628-91aa-7f7bcde85c4f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2394d8ae-8e55-4628-91aa-7f7bcde85c4f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frequncy at which each warehouse appears in our test dataset\n",
        "warehouse_counts = test_df_processed['warehouse'].value_counts().reset_index()\n",
        "warehouse_counts.columns = ['warehouse', 'count']\n",
        "#val=warehouse_counts['warehouse'][0]\n",
        "\n",
        "wr_count = warehouse_counts['count'][warehouse_counts['warehouse'] == 0].item()\n",
        "print(wr_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRgK9tgxz_ZF",
        "outputId": "2a5ac882-a0b7-4c34-8b05-012a46aeb7cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warehouse_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nViU0eAh0C-6",
        "outputId": "20234300-8f36-4d3e-b414-4d7237780286"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   warehouse  count\n",
              "0          4     61\n",
              "1          0     61\n",
              "2          5     61\n",
              "3          6     61\n",
              "4          1     57\n",
              "5          3     48\n",
              "6          2     48"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3aedbf3e-33f6-4bea-8d68-ec66b219d6d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>warehouse</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aedbf3e-33f6-4bea-8d68-ec66b219d6d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3aedbf3e-33f6-4bea-8d68-ec66b219d6d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3aedbf3e-33f6-4bea-8d68-ec66b219d6d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1957c580-45f4-4fde-bca5-d0f08be96d94\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1957c580-45f4-4fde-bca5-d0f08be96d94')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1957c580-45f4-4fde-bca5-d0f08be96d94 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8cd3e155-f085-45cc-8d56-b8442cba40a2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('warehouse_counts')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8cd3e155-f085-45cc-8d56-b8442cba40a2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('warehouse_counts');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "warehouse_counts",
              "summary": "{\n  \"name\": \"warehouse_counts\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"warehouse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          4,\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 48,\n        \"max\": 61,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          61,\n          57,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The most important part: map the known to the unknown:\n",
        "\t•\tThe function is intended to fill in missing or unknown values in the test data (test_df) for a specific feature by using the most recent (latest) values of that feature from the training data (train_df) for each warehouse.\n",
        "\t•\tThis is particularly useful when you want to leverage the most recent patterns or trends from the training data to make better predictions on the test data."
      ],
      "metadata": {
        "id": "pu50Gqdo1A0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the records for extra features for each warehouse and insert into test_df_processed\n",
        "def get_latest_matching_record(train_df, test_df, feature):\n",
        "    # Create a copy of the test dataframe\n",
        "    result_df = test_df.copy()\n",
        "    # Process each warehouse separately\n",
        "    for warehouse in test_df['warehouse'].unique():\n",
        "        # Extract the records for the current warehouse\n",
        "        wr_count = warehouse_counts['count'][warehouse_counts['warehouse'] == warehouse].item()\n",
        "        #print(f'wharehouse {warehouse} occurances in test df: ', wr_count)\n",
        "        last_values = train_df[train_df['warehouse'] == warehouse].tail(wr_count)[feature].values\n",
        "        # Get the rows corresponding to the current warehouse in the result dataframe\n",
        "        warehouse_rows = result_df[result_df['warehouse'] == warehouse].index\n",
        "        # Assign the last wr_count values to the corresponding rows in the result dataframe\n",
        "        for i in range(wr_count):          #(min(wr_count, len(warehouse_rows))):\n",
        "            result_df.loc[warehouse_rows[i], feature] = last_values[i]\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "lQZN9bzx0_RM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_processed= get_latest_matching_record(train_df_processed, test_df_processed,  'orders_holiday')\n",
        "test_df_processed= get_latest_matching_record(train_df_processed, test_df_processed,  'orders_wsh')\n",
        "\n",
        "test_df_processed= get_latest_matching_record(train_df_processed, test_df_processed,  'orders_sh')\n",
        "test_df_processed= get_latest_matching_record(train_df_processed, test_df_processed,  'orders_shops_closed')\n",
        "#test_df_processed= get_latest_matching_record(train_df_processed, test_df_processed, 'daily_avg')\n",
        "#test_df_processed= get_latest_matching_record(train_df_processed, test_df_processed, 'monthly_avg')\n",
        "test_df_processed= get_latest_matching_record(train_df_processed, test_df_processed, 'cumulative_orders')\n",
        "test_df_processed=test_df_processed.fillna(test_df_processed.mean())\n",
        "X = train_df_processed.drop(columns=['orders'])\n",
        "y = train_df_processed['orders']\n",
        "# Show the first few rows of the updated dataset\n",
        "print('train_df_processed.head()', train_df_processed.head())\n",
        "print('test_df_processed.head()', test_df_processed.head())\n",
        "train_df_processed.to_csv('train_data.csv')\n",
        "test_df_processed.to_csv('test_data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNQZrwNh6JtH",
        "outputId": "b3462c5f-73e9-40b0-c374-80a53d867c18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df_processed.head()    warehouse  holiday  shops_closed  winter_school_holidays  school_holidays  \\\n",
            "0          4        0             0                       0                0   \n",
            "1          4        0             0                       0                0   \n",
            "2          4        0             0                       0                0   \n",
            "3          4        0             0                       0                0   \n",
            "4          4        0             0                       0                0   \n",
            "\n",
            "   year  month  day  day_of_week  week_of_year  ...  quarter2  quarter3  \\\n",
            "0  2020     12    5            5            49  ...       4.0       4.0   \n",
            "1  2020     12    6            6            49  ...       4.0       4.0   \n",
            "2  2020     12    7            0            50  ...       4.0       4.0   \n",
            "3  2020     12    8            1            50  ...       4.0       4.0   \n",
            "4  2020     12    9            2            50  ...       4.0       4.0   \n",
            "\n",
            "   quarter4  quarter5  orders  orders_holiday  orders_wsh  orders_sh  \\\n",
            "0       4.0       4.0  6895.0             0.0         0.0        0.0   \n",
            "1       4.0       4.0  6584.0             0.0         0.0        0.0   \n",
            "2       4.0       4.0  7030.0             0.0         0.0        0.0   \n",
            "3       4.0       4.0  6550.0             0.0         0.0        0.0   \n",
            "4       4.0       4.0  6910.0             0.0         0.0        0.0   \n",
            "\n",
            "   orders_shops_closed  cumulative_orders  \n",
            "0                  0.0             6895.0  \n",
            "1                  0.0            13479.0  \n",
            "2                  0.0            20509.0  \n",
            "3                  0.0            27059.0  \n",
            "4                  0.0            33969.0  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "test_df_processed.head()       warehouse  holiday  shops_closed  winter_school_holidays  \\\n",
            "7340          4        0             0                       0   \n",
            "7341          4        0             0                       0   \n",
            "7342          4        0             0                       0   \n",
            "7343          4        0             0                       0   \n",
            "7344          4        0             0                       0   \n",
            "\n",
            "      school_holidays  year  month  day  day_of_week  week_of_year  ...  \\\n",
            "7340                0  2024      3   16            5            11  ...   \n",
            "7341                0  2024      3   17            6            11  ...   \n",
            "7342                0  2024      3   18            0            12  ...   \n",
            "7343                0  2024      3   19            1            12  ...   \n",
            "7344                0  2024      3   20            2            12  ...   \n",
            "\n",
            "      quarter1  quarter2  quarter3  quarter4  quarter5  orders_holiday  \\\n",
            "7340       1.0       1.0       1.0       1.0       1.0             0.0   \n",
            "7341       1.0       1.0       1.0       1.0       1.0             0.0   \n",
            "7342       1.0       1.0       1.0       1.0       1.0             0.0   \n",
            "7343       1.0       1.0       1.0       1.0       1.0             0.0   \n",
            "7344       1.0       1.0       1.0       1.0       1.0             0.0   \n",
            "\n",
            "      orders_wsh  orders_sh  orders_shops_closed  cumulative_orders  \n",
            "7340         0.0        0.0                  0.0          9218982.0  \n",
            "7341         0.0        0.0                  0.0          9228864.0  \n",
            "7342         0.0        0.0                  0.0          9238810.0  \n",
            "7343         0.0        0.0                  0.0          9249450.0  \n",
            "7344         0.0        0.0                  0.0          9261769.0  \n",
            "\n",
            "[5 rows x 84 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The machine learning part: get the strong baseline.\n",
        "In this section, we will get a very powerful baseline. Then, we will do hyperparameters tuning to further optimize the result"
      ],
      "metadata": {
        "id": "m3kLKAtV6cot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "random_seed = 777\n",
        "# Split train data into features and target\n",
        "X = train_df_processed.drop(columns=['orders'])\n",
        "y = train_df_processed['orders']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)"
      ],
      "metadata": {
        "id": "E00agZjR8MlN"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize individual models\n",
        "models = {\n",
        "    'XGBRegressor': XGBRegressor(),\n",
        "    'LGBMRegressor': LGBMRegressor(),\n",
        "    'CatBoostRegressor': CatBoostRegressor(silent=True),\n",
        "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(),\n",
        "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
        "    'RandomForestRegressor': RandomForestRegressor(),\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'Ridge': Ridge(),\n",
        "    'Lasso': Lasso(),\n",
        "    'ElasticNet': ElasticNet(),\n",
        "    'SVR': SVR(),\n",
        "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
        "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
        "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
        "    'BaggingRegressor': BaggingRegressor(),\n",
        "    'SGDRegressor': SGDRegressor(),\n",
        "    'MLPRegressor': MLPRegressor(max_iter=500)\n",
        "}\n",
        "\n",
        "# Dictionary to store evaluation results\n",
        "results = {}\n",
        "\n",
        "# Loop over the models\n",
        "for name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    results[name] = mape\n",
        "\n",
        "# Identifying top 5 models\n",
        "# Sort the dictionary by values (scores) in ascending order and select the top 5\n",
        "top_five_models = dict(sorted(results.items(), key=lambda item: item[1])[:5])\n",
        "\n",
        "# Print the top five models and their scores\n",
        "print(\"Best Performers:\")\n",
        "for model, score in top_five_models.items():\n",
        "    print(f\"{model}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GIm_6HX7cLx",
        "outputId": "8c28630f-0b85-4dd7-e0ed-a48414f2673f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001710 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1135\n",
            "[LightGBM] [Info] Number of data points in the train set: 6973, number of used features: 62\n",
            "[LightGBM] [Info] Start training from score 5540.710455\n",
            "Best Performers:\n",
            "RandomForestRegressor: 0.03407636143179316\n",
            "XGBRegressor: 0.036126134356093756\n",
            "ExtraTreesRegressor: 0.03727163636335777\n",
            "BaggingRegressor: 0.037687897723861954\n",
            "CatBoostRegressor: 0.03901749516021265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize individual models\n",
        "xgb_model = XGBRegressor(random_state=random_seed)\n",
        "cat_model = CatBoostRegressor(random_state=random_seed, silent=True)\n",
        "hgb_model = HistGradientBoostingRegressor(random_state=random_seed)\n",
        "lgb_model = LGBMRegressor(random_state=random_seed)\n",
        "rf_model = RandomForestRegressor(random_state=random_seed)\n",
        "et_model=ExtraTreesRegressor(random_state=random_seed)\n",
        "br_model=BaggingRegressor(random_state=random_seed)\n",
        "dt_model=DecisionTreeRegressor(random_state=random_seed)\n",
        "\n",
        "\n",
        "# Create a VotingRegressor with the models\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('cat', cat_model),\n",
        "        ('hgb', hgb_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('rf', rf_model),\n",
        "        ('et', et_model),\n",
        "        ('br', br_model),\n",
        "        ('dt', dt_model)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the VotingRegressor\n",
        "voting_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on test data\n",
        "y_pred = voting_reg.predict(X_test)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(f'Voting Regressor MAPE: {mape:.4f}')"
      ],
      "metadata": {
        "id": "adwo25Cov19Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53805723-5a60-42e2-fdf8-85c9f1dd18d1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1135\n",
            "[LightGBM] [Info] Number of data points in the train set: 6973, number of used features: 62\n",
            "[LightGBM] [Info] Start training from score 5540.710455\n",
            "Voting Regressor MAPE: 0.0332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the actual test set\n",
        "\n",
        "submit_pred = voting_reg.predict(test_df_processed)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'Target': submit_pred\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission_new1.csv', index=False)\n",
        "print(submission.head(397))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT6Hi1pc9hkm",
        "outputId": "ef8be7a1-d38e-4d5e-a874-5ec0eb651033"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        id        Target\n",
            "0      Prague_1_2024-03-16  10664.245934\n",
            "1      Prague_1_2024-03-17  10389.879176\n",
            "2      Prague_1_2024-03-18   9702.237360\n",
            "3      Prague_1_2024-03-19   9518.588516\n",
            "4      Prague_1_2024-03-20   9553.971586\n",
            "..                     ...           ...\n",
            "392  Budapest_1_2024-05-11   7014.795140\n",
            "393  Budapest_1_2024-05-12   6622.288632\n",
            "394  Budapest_1_2024-05-13   6629.828123\n",
            "395  Budapest_1_2024-05-14   6830.661320\n",
            "396  Budapest_1_2024-05-15   6940.213890\n",
            "\n",
            "[397 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A completely different approach: AutoML"
      ],
      "metadata": {
        "id": "GgqxM0mGARo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autogluon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4DEBgFY4AU5f",
        "outputId": "5d076e9e-e579-477d-af26-20d093d35f0c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.core==1.1.1 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.features==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.tabular==1.1.1 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting autogluon.multimodal==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.multimodal-1.1.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting autogluon.timeseries==1.1.1 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.timeseries-1.1.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.4)\n",
            "Collecting scipy<1.13,>=1.5.4 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.4.1,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.3.2)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.1.4)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7.1)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading boto3-1.35.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ray<2.11,>=2.10.0 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\n",
            "Collecting Pillow<11,>=10.0.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: torch<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.1+cu121)\n",
            "Collecting lightning<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning-2.3.3-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate<0.22.0,>=0.21.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting jsonschema<4.22,>=4.18 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting timm<0.10.0,>=0.9.5 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: torchvision<0.19.0,>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.18.1+cu121)\n",
            "Collecting scikit-image<0.21.0,>=0.19.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\n",
            "Collecting torchmetrics<1.3.0,>=1.2.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pytorch-metric-learning<2.4,>=1.3.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.8.1)\n",
            "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.17.0)\n",
            "Collecting pytesseract<0.3.11,>=0.3.9 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting xgboost<2.1,>=1.6 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.16)\n",
            "Collecting lightgbm<4.4,>=3.3 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: catboost<1.3,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (1.2.5)\n",
            "Requirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\n",
            "Collecting pytorch-lightning<2.4,>=2.2 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting gluonts==0.15.1 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading gluonts-0.15.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading statsforecast-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting mlforecast<0.10.1,>=0.10.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading mlforecast-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting utilsforecast<0.0.11,>=0.0.10 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading utilsforecast-0.0.10-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting orjson~=3.9 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum<1.19,>=1.17 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading optimum-1.18.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (71.0.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.2)\n",
            "Collecting botocore<1.36.0,>=1.35.2 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading botocore-1.35.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (1.16.0)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting dill (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.23.5)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (24.1.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.55)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.20.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.60.0)\n",
            "Collecting window-ops (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (5.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2024.5.15)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.7.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\n",
            "Collecting coloredlogs (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.13.2)\n",
            "Collecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.15.4)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.0.8)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.10.3)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (7.0.4)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading virtualenv-20.26.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.64.1)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (14.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.7.4)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2024.8.10)\n",
            "Collecting PyWavelets>=1.1.1 (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers<4.41.0,>=4.38.0->transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.1.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.3.5)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\n",
            "Collecting pyarrow>=6.0.1 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.43.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (24.3.25)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.20.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.12.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.19.1)\n",
            "Collecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (9.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.63.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.27.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.18.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.6)\n",
            "Collecting filelock (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytz>=2020.1 (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.9)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.6.0)\n",
            "Downloading autogluon-1.1.1-py3-none-any.whl (9.7 kB)\n",
            "Downloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.multimodal-1.1.1-py3-none-any.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.0/428.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.timeseries-1.1.1-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gluonts-0.15.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.3.3-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlforecast-0.10.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.18.1-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Downloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsforecast-1.4.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.0.10-py3-none-any.whl (30 kB)\n",
            "Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.2-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.26.3-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Downloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Downloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=d4c92074872796499c80e72ea73acd729693469e4922d7bb98acb8077c1efc5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=bf5cc59954f48db1c649aa5563a5dec3f267cc20c849804b4c94431ea1f5968f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=1e8bfab08f2c3a432b2756060b493e5622e45f0403d80876b30c8ba4b61a9569\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py3, distlib, colorful, antlr4-python3-runtime, xxhash, virtualenv, tensorboardX, scipy, PyWavelets, pycryptodome, pyarrow, Pillow, orjson, ordered-set, openxlab, onnx, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nptyping, lightning-utilities, jmespath, humanfriendly, dill, colorama, xgboost, window-ops, pytesseract, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, model-index, lightgbm, coloredlogs, botocore, utilsforecast, tokenizers, seqeval, scikit-image, s3transfer, opendatalab, onnxruntime, nvidia-cusolver-cu12, jsonschema, gluonts, aiohttp-cors, transformers, statsforecast, ray, openmim, opencensus, nlpaug, mlforecast, datasets, boto3, torchmetrics, pytorch-metric-learning, evaluate, autogluon.common, accelerate, timm, pytorch-lightning, optimum, autogluon.features, autogluon.core, lightning, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.1\n",
            "    Uninstalling xgboost-2.1.1:\n",
            "      Successfully uninstalled xgboost-2.1.1\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.4.0\n",
            "    Uninstalling lightgbm-4.4.0:\n",
            "      Successfully uninstalled lightgbm-4.4.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.23.2\n",
            "    Uninstalling scikit-image-0.23.2:\n",
            "      Successfully uninstalled scikit-image-0.23.2\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.23.0\n",
            "    Uninstalling jsonschema-4.23.0:\n",
            "      Successfully uninstalled jsonschema-4.23.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.13 requires scikit-image>=0.21.0, but you have scikit-image 0.20.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "osqp 0.6.7.post0 requires scipy!=1.12.0,>=0.13.2, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.4.0 PyWavelets-1.7.0 accelerate-0.21.0 aiohttp-cors-0.7.0 antlr4-python3-runtime-4.9.3 autogluon-1.1.1 autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.multimodal-1.1.1 autogluon.tabular-1.1.1 autogluon.timeseries-1.1.1 boto3-1.35.2 botocore-1.35.2 colorama-0.4.6 coloredlogs-15.0.1 colorful-0.5.6 datasets-2.21.0 dill-0.3.8 distlib-0.3.8 evaluate-0.4.2 gluonts-0.15.1 humanfriendly-10.0 jmespath-1.0.1 jsonschema-4.21.1 lightgbm-4.3.0 lightning-2.3.3 lightning-utilities-0.11.6 mlforecast-0.10.0 model-index-0.1.11 multiprocess-0.70.16 nlpaug-1.1.11 nptyping-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 omegaconf-2.2.3 onnx-1.16.2 onnxruntime-1.19.0 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optimum-1.18.1 ordered-set-4.1.0 orjson-3.10.7 pdf2image-1.17.0 py-spy-0.3.14 pyarrow-17.0.0 pycryptodome-3.20.0 pytesseract-0.3.10 pytorch-lightning-2.3.3 pytorch-metric-learning-2.3.0 ray-2.10.0 s3transfer-0.10.2 scikit-image-0.20.0 scipy-1.12.0 seqeval-1.2.2 statsforecast-1.4.0 tensorboardX-2.6.2.2 timm-0.9.16 tokenizers-0.15.2 torchmetrics-1.2.1 transformers-4.39.3 utilsforecast-0.0.10 virtualenv-20.26.3 window-ops-0.0.15 xgboost-2.0.3 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pyarrow",
                  "pydevd_plugins"
                ]
              },
              "id": "7a6b982ba26e4899bcfd9e5729e36a2c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularPredictor, TabularDataset"
      ],
      "metadata": {
        "id": "1SfAtx3BBbc3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label='orders'\n",
        "train_data =train_df_processed"
      ],
      "metadata": {
        "id": "L_8-a2WICYD3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_percentage_error').fit(\n",
        "    train_data=train_data,\n",
        "    time_limit=6000,  # Train for up to 10 minutes\n",
        "    presets='best_quality',  # Use a good balance of quality and speed\n",
        "    ag_args_fit={'num_gpus': 1},  # Use 1 GPU for training\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiH_Gk-PB2P6",
        "outputId": "cb18aa2c-c043-411b-9bba-58f2d325bb3b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240821_104205\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.88 GB / 12.67 GB (77.9%)\n",
            "Disk Space Avail:   41.42 GB / 78.19 GB (53.0%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 1500s of the 6000s of remaining time (25%).\n",
            "\t\tContext path: \"AutogluonModels/ag-20240821_104205/ds_sub_fit/sub_fit_ho\"\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                     model  score_holdout  score_val                     eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0           XGBoost_BAG_L2      -0.028477  -0.030912  mean_absolute_percentage_error       16.208180      10.823548  1041.669024                 0.435194                0.067421          31.595577            2       True         19\n",
            "1          LightGBM_BAG_L2      -0.028601  -0.030969  mean_absolute_percentage_error       15.966228      10.835953  1046.674969                 0.193241                0.079825          36.601521            2       True         14\n",
            "2      WeightedEnsemble_L3      -0.028663  -0.029490  mean_absolute_percentage_error       16.594657      11.614742  1057.884444                 0.006885                0.005655           0.403677            3       True         21\n",
            "3     ExtraTreesMSE_BAG_L2      -0.028739  -0.029942  mean_absolute_percentage_error       16.160112      11.173200  1023.320323                 0.387125                0.417073          13.246875            2       True         17\n",
            "4   RandomForestMSE_BAG_L2      -0.029195  -0.030559  mean_absolute_percentage_error       16.200646      11.192015  1044.233892                 0.427660                0.435888          34.160444            2       True         15\n",
            "5           XGBoost_BAG_L1      -0.029226  -0.031685  mean_absolute_percentage_error        1.035926       0.142873    64.802108                 1.035926                0.142873          64.802108            1       True          9\n",
            "6      WeightedEnsemble_L2      -0.029306  -0.029891  mean_absolute_percentage_error       12.537602       9.426016   353.825729                 0.010885                0.001060           0.228569            2       True         12\n",
            "7   NeuralNetFastAI_BAG_L2      -0.030174  -0.033758  mean_absolute_percentage_error       16.094074      10.948628  1112.265702                 0.321087                0.192500         102.192255            2       True         18\n",
            "8        LightGBMXT_BAG_L2      -0.030293  -0.033000  mean_absolute_percentage_error       16.287248      10.963609  1045.936139                 0.514261                0.207481          35.862692            2       True         13\n",
            "9     LightGBMLarge_BAG_L1      -0.030457  -0.031922  mean_absolute_percentage_error        2.143654       1.222517    58.414604                 2.143654                1.222517          58.414604            1       True         11\n",
            "10         CatBoost_BAG_L2      -0.030803  -0.032891  mean_absolute_percentage_error       15.868788      10.816339  1122.872416                 0.095802                0.060211         112.798968            2       True         16\n",
            "11         LightGBM_BAG_L1      -0.031532  -0.033320  mean_absolute_percentage_error        1.923029       1.920478    70.537337                 1.923029                1.920478          70.537337            1       True          4\n",
            "12       LightGBMXT_BAG_L1      -0.037319  -0.038409  mean_absolute_percentage_error        7.083612       5.721163   145.812340                 7.083612                5.721163         145.812340            1       True          3\n",
            "13         CatBoost_BAG_L1      -0.038381  -0.041620  mean_absolute_percentage_error        0.369987       0.064731   272.715606                 0.369987                0.064731         272.715606            1       True          6\n",
            "14   NeuralNetTorch_BAG_L2      -0.038723  -0.043719  mean_absolute_percentage_error       15.984716      10.872751  1107.468081                 0.211730                0.116624          97.394634            2       True         20\n",
            "15  RandomForestMSE_BAG_L1      -0.038834  -0.037280  mean_absolute_percentage_error        0.340496       0.417925    14.030770                 0.340496                0.417925          14.030770            1       True          5\n",
            "16    ExtraTreesMSE_BAG_L1      -0.040414  -0.039128  mean_absolute_percentage_error        0.232792       0.620059     9.109524                 0.232792                0.620059           9.109524            1       True          7\n",
            "17   NeuralNetTorch_BAG_L1      -0.047358  -0.054856  mean_absolute_percentage_error        0.175519       0.106308   271.793221                 0.175519                0.106308         271.793221            1       True         10\n",
            "18  NeuralNetFastAI_BAG_L1      -0.052379  -0.060486  mean_absolute_percentage_error        2.223741       0.152807   102.846904                 2.223741                0.152807         102.846904            1       True          8\n",
            "19   KNeighborsUnif_BAG_L1      -0.362060  -0.356638  mean_absolute_percentage_error        0.099901       0.210072     0.005550                 0.099901                0.210072           0.005550            1       True          1\n",
            "20   KNeighborsDist_BAG_L1      -0.394167  -0.389554  mean_absolute_percentage_error        0.144328       0.177193     0.005483                 0.144328                0.177193           0.005483            1       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t1534s\t = DyStack   runtime |\t4466s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 4466s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240821_104205\"\n",
            "Train Data Rows:    7340\n",
            "Train Data Columns: 84\n",
            "Label Column:       orders\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "Warning: dtype UInt32 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'UInt32Dtype()' as a data type\n",
            "\tAvailable Memory:                    9422.04 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.54 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "Warning: dtype UInt32 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'UInt32Dtype()' as a data type\n",
            "Warning: dtype UInt32 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'UInt32Dtype()' as a data type\n",
            "Warning: dtype UInt32 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'UInt32Dtype()' as a data type\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "Warning: dtype UInt32 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'UInt32Dtype()' as a data type\n",
            "\t\t\tNote: Converting 35 features to boolean dtype as they only contain 2 unique values.\n",
            "Warning: dtype UInt32 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'UInt32Dtype()' as a data type\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "Warning: dtype UInt32 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'UInt32Dtype()' as a data type\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 38): ['week_of_year', 'holiday_name_Day of National Unity', 'holiday_name_Memorial Day for the Victims of the Communist Dictatorships', 'holiday_name_National Defense Day', 'holiday_name_Peace Festival in Augsburg', 'holiday_name_Reformation Day', 'year_cos_1', 'year_sin_2', 'year_cos_2', 'year_sin_3', 'year_cos_3', 'year_sin_4', 'year_cos_4', 'year_sin_5', 'year_cos_5', 'month_cos_1', 'month_sin_2', 'month_cos_2', 'month_sin_3', 'month_cos_3', 'month_sin_4', 'month_cos_4', 'month_sin_5', 'month_cos_5', 'day_cos_1', 'day_sin_2', 'day_cos_2', 'day_sin_3', 'day_cos_3', 'day_sin_4', 'day_cos_4', 'day_sin_5', 'day_cos_5', 'quarter1', 'quarter2', 'quarter3', 'quarter4', 'quarter5']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('UInt32', []) :  1 | ['week_of_year']\n",
            "\t\t('float', [])  : 37 | ['holiday_name_Day of National Unity', 'holiday_name_Memorial Day for the Victims of the Communist Dictatorships', 'holiday_name_National Defense Day', 'holiday_name_Peace Festival in Augsburg', 'holiday_name_Reformation Day', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 28 | ['holiday_name_1848 Revolution Memorial Day (Extra holiday)', 'holiday_name_2nd Christmas Day', \"holiday_name_All Saints' Day Holiday\", 'holiday_name_Christmas Eve', 'holiday_name_Cyrila a Metodej', ...]\n",
            "\t\t('int', [])   : 18 | ['warehouse', 'holiday', 'shops_closed', 'winter_school_holidays', 'school_holidays', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     :  8 | ['year_sin_1', 'month_sin_1', 'day_sin_1', 'orders_holiday', 'orders_wsh', ...]\n",
            "\t\t('int', [])       :  8 | ['warehouse', 'year', 'month', 'day', 'day_of_week', ...]\n",
            "\t\t('int', ['bool']) : 30 | ['holiday', 'shops_closed', 'winter_school_holidays', 'school_holidays', 'is_month_start', ...]\n",
            "\t2.9s = Fit runtime\n",
            "\t46 features in original data used to generate 46 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.97 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 2.94s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_percentage_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 108 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2974.33s of the 4462.59s of remaining time.\n",
            "\t-0.3624\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.31s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2973.99s of the 4462.26s of remaining time.\n",
            "\t-0.3934\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2973.66s of the 4461.92s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.15%)\n",
            "\t-0.037\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t155.84s\t = Training   runtime\n",
            "\t7.57s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2814.13s of the 4302.4s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.15%)\n",
            "\t-0.0323\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t79.53s\t = Training   runtime\n",
            "\t2.63s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2732.05s of the 4220.32s of remaining time.\n",
            "\t-0.0366\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t14.98s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2716.2s of the 4204.47s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.24%)\n",
            "\t-0.0399\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t351.14s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2362.76s of the 3851.03s of remaining time.\n",
            "\t-0.0382\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t9.84s\t = Training   runtime\n",
            "\t0.38s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2352.13s of the 3840.39s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.12%)\n",
            "\t-0.0545\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t105.99s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2244.41s of the 3732.68s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.28%)\n",
            "\t-0.0308\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t71.19s\t = Training   runtime\n",
            "\t0.19s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2171.09s of the 3659.36s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
            "\t-0.0432\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t515.83s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1653.46s of the 3141.72s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.44%)\n",
            "\t-0.0309\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t79.74s\t = Training   runtime\n",
            "\t1.79s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1569.97s of the 3058.23s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.24%)\n",
            "\t-0.0402\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t238.02s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1330.05s of the 2818.32s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
            "\t-0.0498\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t431.78s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 896.24s of the 2384.51s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.22%)\n",
            "\t-0.0311\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t171.13s\t = Training   runtime\n",
            "\t6.51s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 721.28s of the 2209.54s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.12%)\n",
            "\t-0.063\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t132.85s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 586.36s of the 2074.62s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.80%)\n",
            "\t-0.0306\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t263.83s\t = Training   runtime\n",
            "\t0.55s\t = Validation runtime\n",
            "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 318.69s of the 1806.96s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.11%)\n",
            "\t-0.0426\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t168.24s\t = Training   runtime\n",
            "\t7.54s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 146.7s of the 1634.97s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
            "\t-0.0756\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t147.57s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1484.41s of remaining time.\n",
            "\tEnsemble Weights: {'XGBoost_BAG_L1': 0.32, 'LightGBMLarge_BAG_L1': 0.16, 'RandomForestMSE_BAG_L1': 0.12, 'LightGBM_r131_BAG_L1': 0.12, 'CatBoost_r9_BAG_L1': 0.12, 'LightGBM_BAG_L1': 0.08, 'LightGBMXT_BAG_L1': 0.04, 'NeuralNetTorch_BAG_L1': 0.04}\n",
            "\t-0.0289\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t0.37s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1484.02s of the 1483.91s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.23%)\n",
            "\t-0.0312\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t42.91s\t = Training   runtime\n",
            "\t0.36s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1439.08s of the 1438.96s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.22%)\n",
            "\t-0.0294\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t39.17s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1397.92s of the 1397.81s of remaining time.\n",
            "\t-0.0289\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t52.33s\t = Training   runtime\n",
            "\t0.44s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1344.71s of the 1344.6s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.34%)\n",
            "\t-0.0316\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t80.59s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1262.41s of the 1262.3s of remaining time.\n",
            "\t-0.0286\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t15.41s\t = Training   runtime\n",
            "\t0.45s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1246.1s of the 1245.99s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.17%)\n",
            "\t-0.0317\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t104.61s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1139.72s of the 1139.62s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.39%)\n",
            "\t-0.0292\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t31.91s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1104.72s of the 1104.61s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.10%)\n",
            "\t-0.0487\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t99.42s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1003.49s of the 1003.39s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.62%)\n",
            "\t-0.0296\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t60.04s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 941.2s of the 941.09s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.34%)\n",
            "\t-0.0321\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t64.27s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 875.05s of the 874.93s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.10%)\n",
            "\t-0.037\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t218.22s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 654.14s of the 654.03s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.32%)\n",
            "\t-0.0294\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t58.3s\t = Training   runtime\n",
            "\t0.57s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 593.58s of the 593.47s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.18%)\n",
            "\t-0.0315\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t136.84s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 454.68s of the 454.57s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.12%)\n",
            "\t-0.0304\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t72.22s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 380.14s of the 380.03s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.16%)\n",
            "\t-0.0307\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t89.28s\t = Training   runtime\n",
            "\t1.97s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 288.67s of the 288.56s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.10%)\n",
            "\t-0.0314\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t198.81s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 87.93s of the 87.82s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.18%)\n",
            "\t-0.0293\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t70.65s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 15.08s of the 14.97s of remaining time.\n",
            "\t-0.0287\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t12.36s\t = Training   runtime\n",
            "\t0.45s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 1.09s of remaining time.\n",
            "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.2, 'XGBoost_BAG_L1': 0.16, 'RandomForestMSE_BAG_L2': 0.12, 'XGBoost_BAG_L2': 0.12, 'XGBoost_r33_BAG_L2': 0.12, 'ExtraTrees_r42_BAG_L2': 0.08, 'RandomForestMSE_BAG_L1': 0.04, 'LightGBMLarge_BAG_L1': 0.04, 'CatBoost_r9_BAG_L1': 0.04, 'NeuralNetFastAI_BAG_L2': 0.04, 'NeuralNetTorch_r22_BAG_L2': 0.04}\n",
            "\t-0.0282\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t0.32s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 4464.82s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 32.3 rows/s (918 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240821_104205\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = predictor.fit_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO0DDRAWEz8X",
        "outputId": "d413a0b4-e308-4a7c-9b70-a2da66986427"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                          model  score_val                     eval_metric  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0           WeightedEnsemble_L3  -0.028206  mean_absolute_percentage_error      30.802607  3423.919656                0.000781           0.316911            3       True         38\n",
            "1          ExtraTreesMSE_BAG_L2  -0.028633  mean_absolute_percentage_error      29.450267  2952.920506                0.454630          15.407227            2       True         24\n",
            "2         ExtraTrees_r42_BAG_L2  -0.028708  mean_absolute_percentage_error      29.446817  2949.871347                0.451180          12.358068            2       True         37\n",
            "3           WeightedEnsemble_L2  -0.028875  mean_absolute_percentage_error      19.764074  1352.443174                0.001132           0.369147            2       True         19\n",
            "4        RandomForestMSE_BAG_L2  -0.028885  mean_absolute_percentage_error      29.436772  2989.847861                0.441135          52.334582            2       True         22\n",
            "5                XGBoost_BAG_L2  -0.029231  mean_absolute_percentage_error      29.052401  2969.426234                0.056764          31.912955            2       True         26\n",
            "6            XGBoost_r33_BAG_L2  -0.029330  mean_absolute_percentage_error      29.129620  3008.161448                0.133983          70.648169            2       True         36\n",
            "7          LightGBM_r131_BAG_L2  -0.029380  mean_absolute_percentage_error      29.568305  2995.810398                0.572668          58.297119            2       True         31\n",
            "8               LightGBM_BAG_L2  -0.029403  mean_absolute_percentage_error      29.080214  2976.681092                0.084577          39.167813            2       True         21\n",
            "9          LightGBMLarge_BAG_L2  -0.029645  mean_absolute_percentage_error      29.194644  2997.553921                0.199007          60.040641            2       True         28\n",
            "10           CatBoost_r9_BAG_L2  -0.030419  mean_absolute_percentage_error      29.114604  3009.735305                0.118968          72.222026            2       True         33\n",
            "11           CatBoost_r9_BAG_L1  -0.030632  mean_absolute_percentage_error       0.550207   263.833567                0.550207         263.833567            1       True         16\n",
            "12          LightGBM_r96_BAG_L2  -0.030689  mean_absolute_percentage_error      30.966664  3026.797015                1.971028          89.283736            2       True         34\n",
            "13               XGBoost_BAG_L1  -0.030840  mean_absolute_percentage_error       0.194235    71.187298                0.194235          71.187298            1       True          9\n",
            "14         LightGBMLarge_BAG_L1  -0.030872  mean_absolute_percentage_error       1.792684    79.742987                1.792684          79.742987            1       True         11\n",
            "15         LightGBM_r131_BAG_L1  -0.031102  mean_absolute_percentage_error       6.511811   171.129564                6.511811         171.129564            1       True         14\n",
            "16            LightGBMXT_BAG_L2  -0.031189  mean_absolute_percentage_error      29.352542  2980.424225                0.356905          42.910946            2       True         20\n",
            "17    NeuralNetTorch_r22_BAG_L2  -0.031407  mean_absolute_percentage_error      29.104872  3136.327165                0.109236         198.813886            2       True         35\n",
            "18  NeuralNetFastAI_r191_BAG_L2  -0.031470  mean_absolute_percentage_error      29.167916  3074.354766                0.172280         136.841487            2       True         32\n",
            "19              CatBoost_BAG_L2  -0.031634  mean_absolute_percentage_error      29.056352  3018.103498                0.060715          80.590219            2       True         23\n",
            "20       NeuralNetFastAI_BAG_L2  -0.031734  mean_absolute_percentage_error      29.154898  3042.127859                0.159261         104.614580            2       True         25\n",
            "21         CatBoost_r177_BAG_L2  -0.032137  mean_absolute_percentage_error      29.048874  3001.780013                0.053237          64.266733            2       True         29\n",
            "22              LightGBM_BAG_L1  -0.032283  mean_absolute_percentage_error       2.629996    79.533633                2.629996          79.533633            1       True          4\n",
            "23       RandomForestMSE_BAG_L1  -0.036625  mean_absolute_percentage_error       0.417668    14.981901                0.417668          14.981901            1       True          5\n",
            "24            LightGBMXT_BAG_L1  -0.036982  mean_absolute_percentage_error       7.572806   155.837388                7.572806         155.837388            1       True          3\n",
            "25    NeuralNetTorch_r79_BAG_L2  -0.036988  mean_absolute_percentage_error      29.116432  3155.730877                0.120796         218.217598            2       True         30\n",
            "26         ExtraTreesMSE_BAG_L1  -0.038215  mean_absolute_percentage_error       0.380222     9.844617                0.380222           9.844617            1       True          7\n",
            "27              CatBoost_BAG_L1  -0.039899  mean_absolute_percentage_error       0.092901   351.140314                0.092901         351.140314            1       True          6\n",
            "28         CatBoost_r177_BAG_L1  -0.040201  mean_absolute_percentage_error       0.067147   238.015059                0.067147         238.015059            1       True         12\n",
            "29          LightGBM_r96_BAG_L1  -0.042562  mean_absolute_percentage_error       7.538860   168.235523                7.538860         168.235523            1       True         17\n",
            "30        NeuralNetTorch_BAG_L1  -0.043224  mean_absolute_percentage_error       0.093536   515.827688                0.093536         515.827688            1       True         10\n",
            "31        NeuralNetTorch_BAG_L2  -0.048662  mean_absolute_percentage_error      29.102448  3036.937018                0.106812          99.423739            2       True         27\n",
            "32    NeuralNetTorch_r79_BAG_L1  -0.049839  mean_absolute_percentage_error       0.096063   431.776528                0.096063         431.776528            1       True         13\n",
            "33       NeuralNetFastAI_BAG_L1  -0.054466  mean_absolute_percentage_error       0.164710   105.985479                0.164710         105.985479            1       True          8\n",
            "34  NeuralNetFastAI_r191_BAG_L1  -0.062971  mean_absolute_percentage_error       0.161857   132.853575                0.161857         132.853575            1       True         15\n",
            "35    NeuralNetTorch_r22_BAG_L1  -0.075577  mean_absolute_percentage_error       0.115389   147.574259                0.115389         147.574259            1       True         18\n",
            "36        KNeighborsUnif_BAG_L1  -0.362449  mean_absolute_percentage_error       0.313845     0.005818                0.313845           0.005818            1       True          1\n",
            "37        KNeighborsDist_BAG_L1  -0.393365  mean_absolute_percentage_error       0.301701     0.008081                0.301701           0.008081            1       True          2\n",
            "Number of models trained: 38\n",
            "Types of models trained:\n",
            "{'StackerEnsembleModel_LGB', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_XGBoost', 'WeightedEnsembleModel', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_NNFastAiTabular'}\n",
            "Bagging used: True  (with 8 folds)\n",
            "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     :  8 | ['year_sin_1', 'month_sin_1', 'day_sin_1', 'orders_holiday', 'orders_wsh', ...]\n",
            "('int', [])       :  8 | ['warehouse', 'year', 'month', 'day', 'day_of_week', ...]\n",
            "('int', ['bool']) : 30 | ['holiday', 'shops_closed', 'winter_school_holidays', 'school_holidays', 'is_month_start', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240821_104205SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predictor.predict(test_df_processed,model=\"ExtraTrees_r42_BAG_L2\")\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'Target': predictions.reset_index(drop=\"index\")\n",
        "})\n",
        "submission.to_csv('submission_f3.csv', index=False)"
      ],
      "metadata": {
        "id": "w7QOHbhjDjJi"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We didn't see much of the performance grow. Maybe we shrink the dataset dimensions to remove the noise."
      ],
      "metadata": {
        "id": "Hq0AZZmnVE6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq-Q-EPxeY_y",
        "outputId": "4b0f3d48-d33a-4812-a1cc-9b41e8e67fec"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7340, 85)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming `train_df` is your DataFrame containing the training data\n",
        "X = train_data.drop(columns=['orders'])  # Replace 'orders' with your target column name\n",
        "y = train_data['orders']  # Target variable\n",
        "\n",
        "# Initialize and train the RandomForest model\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Use SelectFromModel to select the most important features\n",
        "selector = SelectFromModel(rf, threshold='2*median', prefit=True)\n",
        "selected_features = selector.transform(X)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X.columns[selector.get_support()]\n",
        "\n",
        "# Create a DataFrame with the selected features\n",
        "important_features_df = pd.DataFrame(selected_features, columns=selected_feature_names)\n",
        "\n",
        "# Optionally, combine the selected features with the target variable\n",
        "final_df = pd.concat([important_features_df, y.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Output the selected features and their corresponding DataFrame\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)\n",
        "\n",
        "print(\"\\nFinal DataFrame with Selected Features:\")\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma9FS0DgYa8v",
        "outputId": "e504c86a-8ae3-43d1-de98-5a3f7e3ca675"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features:\n",
            "Index(['warehouse', 'holiday', 'year', 'month', 'day', 'day_of_week',\n",
            "       'week_of_year', 'quarter', 'holiday_before', 'holiday_after',\n",
            "       'total_holidays_month', 'total_shops_closed_week',\n",
            "       'holiday_name_Christmas Eve', 'holiday_name_New Years Day',\n",
            "       'year_cos_1', 'year_cos_2', 'year_cos_3', 'year_cos_4', 'year_cos_5',\n",
            "       'month_cos_1', 'month_cos_2', 'month_cos_3', 'month_cos_4',\n",
            "       'month_cos_5', 'day_cos_1', 'day_cos_2', 'day_cos_3', 'day_cos_4',\n",
            "       'day_cos_5', 'quarter5', 'orders_holiday', 'orders_wsh',\n",
            "       'cumulative_orders'],\n",
            "      dtype='object')\n",
            "\n",
            "Final DataFrame with Selected Features:\n",
            "   warehouse  holiday    year  month  day  day_of_week  week_of_year  quarter  \\\n",
            "0        4.0      0.0  2020.0   12.0  5.0          5.0          49.0      4.0   \n",
            "1        4.0      0.0  2020.0   12.0  6.0          6.0          49.0      4.0   \n",
            "2        4.0      0.0  2020.0   12.0  7.0          0.0          50.0      4.0   \n",
            "3        4.0      0.0  2020.0   12.0  8.0          1.0          50.0      4.0   \n",
            "4        4.0      0.0  2020.0   12.0  9.0          2.0          50.0      4.0   \n",
            "\n",
            "   holiday_before  holiday_after  ...  day_cos_1  day_cos_2  day_cos_3  \\\n",
            "0             0.0            0.0  ...        5.0        5.0        5.0   \n",
            "1             0.0            0.0  ...        6.0        6.0        6.0   \n",
            "2             0.0            0.0  ...        7.0        7.0        7.0   \n",
            "3             0.0            0.0  ...        8.0        8.0        8.0   \n",
            "4             0.0            0.0  ...        9.0        9.0        9.0   \n",
            "\n",
            "   day_cos_4  day_cos_5  quarter5  orders_holiday  orders_wsh  \\\n",
            "0        5.0        5.0       4.0             0.0         0.0   \n",
            "1        6.0        6.0       4.0             0.0         0.0   \n",
            "2        7.0        7.0       4.0             0.0         0.0   \n",
            "3        8.0        8.0       4.0             0.0         0.0   \n",
            "4        9.0        9.0       4.0             0.0         0.0   \n",
            "\n",
            "   cumulative_orders  orders  \n",
            "0             6895.0  6895.0  \n",
            "1            13479.0  6584.0  \n",
            "2            20509.0  7030.0  \n",
            "3            27059.0  6550.0  \n",
            "4            33969.0  6910.0  \n",
            "\n",
            "[5 rows x 34 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the test data using the already fitted selector\n",
        "selected_test_features = selector.transform(test_df_processed)\n",
        "\n",
        "# Create a DataFrame with the selected features from the test set\n",
        "important_test_features_df = pd.DataFrame(selected_test_features, columns=selected_feature_names)\n",
        "\n",
        "# Output the selected features and their corresponding DataFrame\n",
        "print(\"Selected Features in Test Set:\")\n",
        "print(selected_feature_names)\n",
        "\n",
        "print(\"\\nFinal Test DataFrame with Selected Features:\")\n",
        "print(important_test_features_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPlKWWcIZkIV",
        "outputId": "28dfb0a5-4905-4792-9605-330dc229e327"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features in Test Set:\n",
            "Index(['warehouse', 'holiday', 'year', 'month', 'day', 'day_of_week',\n",
            "       'week_of_year', 'quarter', 'holiday_before', 'holiday_after',\n",
            "       'total_holidays_month', 'total_shops_closed_week',\n",
            "       'holiday_name_Christmas Eve', 'holiday_name_New Years Day',\n",
            "       'year_cos_1', 'year_cos_2', 'year_cos_3', 'year_cos_4', 'year_cos_5',\n",
            "       'month_cos_1', 'month_cos_2', 'month_cos_3', 'month_cos_4',\n",
            "       'month_cos_5', 'day_cos_1', 'day_cos_2', 'day_cos_3', 'day_cos_4',\n",
            "       'day_cos_5', 'quarter5', 'orders_holiday', 'orders_wsh',\n",
            "       'cumulative_orders'],\n",
            "      dtype='object')\n",
            "\n",
            "Final Test DataFrame with Selected Features:\n",
            "   warehouse  holiday    year  month   day  day_of_week  week_of_year  \\\n",
            "0        4.0      0.0  2024.0    3.0  16.0          5.0          11.0   \n",
            "1        4.0      0.0  2024.0    3.0  17.0          6.0          11.0   \n",
            "2        4.0      0.0  2024.0    3.0  18.0          0.0          12.0   \n",
            "3        4.0      0.0  2024.0    3.0  19.0          1.0          12.0   \n",
            "4        4.0      0.0  2024.0    3.0  20.0          2.0          12.0   \n",
            "\n",
            "   quarter  holiday_before  holiday_after  ...  month_cos_5  day_cos_1  \\\n",
            "0      1.0             0.0            0.0  ...          3.0       16.0   \n",
            "1      1.0             0.0            0.0  ...          3.0       17.0   \n",
            "2      1.0             0.0            0.0  ...          3.0       18.0   \n",
            "3      1.0             0.0            0.0  ...          3.0       19.0   \n",
            "4      1.0             0.0            0.0  ...          3.0       20.0   \n",
            "\n",
            "   day_cos_2  day_cos_3  day_cos_4  day_cos_5  quarter5  orders_holiday  \\\n",
            "0       16.0       16.0       16.0       16.0       1.0             0.0   \n",
            "1       17.0       17.0       17.0       17.0       1.0             0.0   \n",
            "2       18.0       18.0       18.0       18.0       1.0             0.0   \n",
            "3       19.0       19.0       19.0       19.0       1.0             0.0   \n",
            "4       20.0       20.0       20.0       20.0       1.0             0.0   \n",
            "\n",
            "   orders_wsh  cumulative_orders  \n",
            "0         0.0          9218982.0  \n",
            "1         0.0          9228864.0  \n",
            "2         0.0          9238810.0  \n",
            "3         0.0          9249450.0  \n",
            "4         0.0          9261769.0  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "random_seed = 777\n",
        "# Split train data into features and target\n",
        "X = final_df.drop(columns=['orders'])\n",
        "y = final_df['orders']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)"
      ],
      "metadata": {
        "id": "LLDEKtq7ZsEY"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize individual models\n",
        "xgb_model = XGBRegressor(random_state=random_seed)\n",
        "cat_model = CatBoostRegressor(random_state=random_seed, silent=True)\n",
        "hgb_model = HistGradientBoostingRegressor(random_state=random_seed)\n",
        "lgb_model = LGBMRegressor(random_state=random_seed)\n",
        "rf_model = RandomForestRegressor(random_state=random_seed)\n",
        "et_model=ExtraTreesRegressor(random_state=random_seed)\n",
        "br_model=BaggingRegressor(random_state=random_seed)\n",
        "dt_model=DecisionTreeRegressor(random_state=random_seed)\n",
        "\n",
        "\n",
        "# Create a VotingRegressor with the models\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('cat', cat_model),\n",
        "        ('hgb', hgb_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('rf', rf_model),\n",
        "        ('et', et_model),\n",
        "        ('br', br_model),\n",
        "        ('dt', dt_model)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the VotingRegressor\n",
        "voting_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on test data\n",
        "y_pred = voting_reg.predict(X_test)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(f'Voting Regressor MAPE: {mape:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVN5OI_EZ5cR",
        "outputId": "00c2211a-600a-4439-f917-9823e2c1b7fd"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 772\n",
            "[LightGBM] [Info] Number of data points in the train set: 5872, number of used features: 31\n",
            "[LightGBM] [Info] Start training from score 5542.194142\n",
            "Voting Regressor MAPE: 0.0339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the actual test set\n",
        "\n",
        "submit_pred = voting_reg.predict(important_test_features_df)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'Target': submit_pred\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission5.csv', index=False)\n",
        "print(submission.head(397))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjluZ-Q3aaP1",
        "outputId": "f19a3197-840d-4af5-8b44-c1678cc8c533"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        id        Target\n",
            "0      Prague_1_2024-03-16  10734.446381\n",
            "1      Prague_1_2024-03-17  10419.622697\n",
            "2      Prague_1_2024-03-18   9871.772180\n",
            "3      Prague_1_2024-03-19   9663.365009\n",
            "4      Prague_1_2024-03-20   9648.252073\n",
            "..                     ...           ...\n",
            "392  Budapest_1_2024-05-11   6904.956695\n",
            "393  Budapest_1_2024-05-12   6334.302450\n",
            "394  Budapest_1_2024-05-13   6512.297098\n",
            "395  Budapest_1_2024-05-14   6550.312603\n",
            "396  Budapest_1_2024-05-15   6549.551096\n",
            "\n",
            "[397 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try finetuning one single model. See how good it can get"
      ],
      "metadata": {
        "id": "AktXeDqG-_Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcuyNdnECWZG",
        "outputId": "ebcf7d24-05a5-4586-d1c7-8eb19175d94e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-24.7.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.7.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "random_seed = 777\n",
        "# Split train data into features and target\n",
        "X = train_df_processed.drop(columns=['orders'])\n",
        "y = train_df_processed['orders']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=random_seed)"
      ],
      "metadata": {
        "id": "8HGGbs73-75y"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.metrics import make_scorer, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize the XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42,tree_method='gpu_hist')\n",
        "\n",
        "# Define the parameter space for Bayesian optimization\n",
        "param_space = {\n",
        "    'n_estimators': (100, 1000),\n",
        "    'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "    'max_depth': (3, 10),\n",
        "    'min_child_weight': (1, 10),\n",
        "    'gamma': (0.0, 1.0, 'uniform'),\n",
        "    'subsample': (0.6, 1.0, 'uniform'),\n",
        "    'colsample_bytree': (0.6, 1.0, 'uniform'),\n",
        "    'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
        "    'reg_lambda': (1e-9, 1.0, 'log-uniform')\n",
        "}\n",
        "\n",
        "# Create a custom scorer for MAPE\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "# Initialize BayesSearchCV\n",
        "bayes_search = BayesSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    search_spaces=param_space,\n",
        "    n_iter=50,  # Number of parameter settings to sample\n",
        "    scoring=mape_scorer,  # Use MAPE as the scoring metric\n",
        "    cv=5,  # Number of cross-validation folds\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all available cores\n",
        ")\n",
        "\n",
        "# Fit BayesSearchCV to find the best parameters\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(f\"Best parameters found: {bayes_search.best_params_}\")\n",
        "print(f\"Best score (MAPE): {abs(bayes_search.best_score_):.4f}\")\n",
        "\n",
        "# Evaluate the model with the best parameters on the validation set\n",
        "best_model = bayes_search.best_estimator_\n",
        "y_pred = best_model.predict(X_val)\n",
        "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
        "print(f\"Validation MAPE: {mape:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH0XSi85CZN3",
        "outputId": "22670d01-304e-42d5-a660-e7f468b4ec61"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Best parameters found: OrderedDict([('colsample_bytree', 0.9654749821766092), ('gamma', 0.1613732400087867), ('learning_rate', 0.07074312110709176), ('max_depth', 7), ('min_child_weight', 2), ('n_estimators', 547), ('reg_alpha', 1e-09), ('reg_lambda', 1.0), ('subsample', 0.7494894323932338)])\n",
            "Best score (MAPE): 0.0333\n",
            "Validation MAPE: 0.0318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the actual test set\n",
        "submit_pred = best_model.predict(test_df_processed)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'Target': submit_pred\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission_single_best_model.csv', index=False)\n",
        "print(submission.head(397))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dytG1GWSAULM",
        "outputId": "eac59489-2955-4f0b-ca34-e63e6aec5f14"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        id        Target\n",
            "0      Prague_1_2024-03-16  10539.750000\n",
            "1      Prague_1_2024-03-17  10352.125000\n",
            "2      Prague_1_2024-03-18   9857.996094\n",
            "3      Prague_1_2024-03-19   9647.023438\n",
            "4      Prague_1_2024-03-20   9597.051758\n",
            "..                     ...           ...\n",
            "392  Budapest_1_2024-05-11   7199.659180\n",
            "393  Budapest_1_2024-05-12   6539.994629\n",
            "394  Budapest_1_2024-05-13   6582.928223\n",
            "395  Budapest_1_2024-05-14   6618.795898\n",
            "396  Budapest_1_2024-05-15   6464.374023\n",
            "\n",
            "[397 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# let's try tune the parameters of our stacking model. (mannully)"
      ],
      "metadata": {
        "id": "POeplk9VHfOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor, RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor  # Correct import\n",
        "import numpy as np\n",
        "\n",
        "# Initialize individual models with GPU support where applicable\n",
        "xgb_model = xgb.XGBRegressor(random_state=random_seed, tree_method='gpu_hist')\n",
        "cat_model = CatBoostRegressor(random_state=random_seed, silent=True, task_type=\"GPU\")\n",
        "hgb_model = HistGradientBoostingRegressor(random_state=random_seed)\n",
        "lgb_model = LGBMRegressor(random_state=random_seed, n_jobs=-1)\n",
        "rf_model = RandomForestRegressor(random_state=random_seed)\n",
        "et_model = ExtraTreesRegressor(random_state=random_seed)\n",
        "br_model = BaggingRegressor(random_state=random_seed)\n",
        "dt_model = DecisionTreeRegressor(random_state=random_seed)\n",
        "\n",
        "# Voting Regressor without tuning\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('cat', cat_model),\n",
        "        ('hgb', hgb_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('rf', rf_model),\n",
        "        ('et', et_model),\n",
        "        ('br', br_model),\n",
        "        ('dt', dt_model),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define parameter grid for Bayesian Optimization\n",
        "param_grid = {\n",
        "    'xgb__n_estimators': (100, 1000),\n",
        "    'xgb__max_depth': (3, 10),\n",
        "    'xgb__learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "    'xgb__subsample': (0.6, 1.0),\n",
        "    'cat__depth': (3, 10),\n",
        "    'cat__learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "    'cat__l2_leaf_reg': (1, 10),\n",
        "    'hgb__max_iter': (100, 1000),\n",
        "    'hgb__learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "    'hgb__max_depth': (3, 10),\n",
        "    'lgb__n_estimators': (100, 1000),\n",
        "    'lgb__max_depth': (-1, 10),\n",
        "    'lgb__learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "    'lgb__subsample': (0.6, 1.0),\n",
        "    'rf__n_estimators': (100, 1000),\n",
        "    'rf__max_depth': (3, 20),\n",
        "    'et__n_estimators': (100, 1000),\n",
        "    'et__max_depth': (3, 20),\n",
        "    'br__n_estimators': (10, 100),\n",
        "    'dt__max_depth': (3, 20),\n",
        "}\n",
        "\n",
        "# Bayesian search\n",
        "opt = BayesSearchCV(\n",
        "    estimator=voting_reg,\n",
        "    search_spaces=param_grid,\n",
        "    n_iter=50,  # Increased number of iterations for more thorough search\n",
        "    cv=KFold(n_splits=5),\n",
        "    scoring='neg_mean_absolute_percentage_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=random_seed\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "opt.fit(X_train, y_train)\n",
        "\n",
        "# Best model evaluation\n",
        "best_model = opt.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(f'Optimized Voting Regressor MAPE: {mape:.4f}')\n",
        "print(f'Best parameters found: {opt.best_params_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMZIkIkYIVzl",
        "outputId": "e5845ee8-8e0c-4175-a777-af4159501515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the actual test set\n",
        "submit_pred = best_model.predict(test_df_processed)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'Target': submit_pred\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission_stacking_best_model.csv', index=False)\n",
        "print(submission.head(397))"
      ],
      "metadata": {
        "id": "vKC-iAopJzYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}